@c Copyright (c) 2004 Free Software Foundation, Inc.
@c Free Software Foundation, Inc.
@c This is part of the GCC manual.
@c For copying conditions, see the file gcc.texi.

@c ---------------------------------------------------------------------
@c Tree SSA
@c ---------------------------------------------------------------------

@node Tree SSA
@chapter Analysis and Optimization of GIMPLE Trees
@cindex Tree SSA
@cindex Optimization infrastructure for GIMPLE

GCC uses three main intermediate languages to represent the program
during compilation: GENERIC, GIMPLE and RTL.  GENERIC is a
language-independent representation generated by each front end.  It
is used to serve as an interface between the parser and optimizer.
GENERIC is a common representation that is able to represent programs
written in all the languages supported by GCC.

GIMPLE and RTL are used to optimize the program.  GIMPLE is used for
target and language independent optimizations (e.g., inlining,
constant propagation, tail call elimination, redundancy elimination,
etc).  Much like GENERIC, GIMPLE is a language independent, tree based
representation. However, it differs from GENERIC in that the GIMPLE
grammar is more restrictive: expressions contain no more than 3
operands (except function calls), it has no control flow structures
and expressions with side-effects are only allowed on the right hand
side of assignments.  See the chapter describing GENERIC and GIMPLE
for more details.

This chapter describes the data structures and functions used in the
GIMPLE optimizers (also known as ``tree optimizers'' or ``middle
end'').  In particular, it focuses on all the macros, data structures,
functions and programming constructs needed to implement optimization
passes for GIMPLE.

@menu
* Annotations::		Attributes for statements and variables.
* Statement Operands::	Variables referenced by GIMPLE statements.
* SSA::			Static Single Assignment representation.
* Alias analysis::	Representing aliased loads and stores.
@end menu

@node Annotations
@section Annotations
@cindex annotations

The optimizers need to associate attributes with statements and
variables during the optimization process.  For instance, we need to
know what basic block does a statement belong to or whether a variable
has aliases.  All these attributes are stored in data structures
called annotations which are then linked to the field @code{ann} in
@code{struct tree_common}.

Presently, we define annotations for statements (@code{stmt_ann_t}),
variables (@code{var_ann_t}) and SSA names (@code{ssa_name_ann_t}).
Annotations are defined and documented in @file{tree-flow.h}.


@node Statement Operands
@section Statement Operands
@cindex operands
@cindex virtual operands
@cindex real operands
@findex get_stmt_operands
@findex modify_stmt

Almost every GIMPLE statement will contain a reference to a variable
or memory location.  Since statements come in different shapes and
sizes, their operands are going to be located at various spots inside
the statement's tree.  To facilitate access to the statement's
operands, they are organized into arrays associated inside each
statement's annotation.  Each element in an operand array is a pointer
to a @code{VAR_DECL}, @code{PARM_DECL} or @code{SSA_NAME} tree node.
This provides a very convenient way of examining and replacing
operands.  

Data flow analysis and optimization is done on all tree nodes
representing variables.  Any node for which @code{SSA_VAR_P} returns
nonzero is considered when scanning statement operands.  However, not
all @code{SSA_VAR_P} variables are processed in the same way.  For the
purposes of optimization, we need to distinguish between references to
local scalar variables and references to globals, statics, structures,
arrays, aliased variables, etc.  The reason is simple, the compiler
can gather complete data flow information for a local scalar.  On the
other hand, a global variable may be modified by a function call, it
may not be possible to keep track of all the elements of an array or
the fields of a structure, etc.

The operand scanner gathers two kinds of operands: @dfn{real} and
@dfn{virtual}.  An operand for which @code{is_gimple_reg} returns true
is considered real, otherwise it is a virtual operand.  We also
distinguish between uses and definitions.  An operand is used if its
value is loaded by the statement (e.g., the operand at the RHS of an
assignment).  If the statement assigns a new value to the operand, the
operand it is considered a definition (e.g., the operand at the LHS of
an assignment).

Virtual and real operands also have very different data flow
properties.  Real operands are unambiguous references to the
full object that they represent.  For instance, given

@smallexample
@{
  int a, b;
  a = b
@}
@end smallexample

Since @code{a} and @code{b} are non-aliased locals, the statement
@code{a = b} will have one real definition and one real use because
variable @code{b} is completely modified with the contents of
variable @code{a}.  Real definition are also known as @dfn{killing
definitions}.  Similarly, the use of @code{a} reads all its bits.

In contrast, virtual operands represent partial or ambiguous
references to a variable.  For instance, given

@smallexample
@{
  int a, b, *p;

  if (...)
    p = &a;
  else
    p = &b;
  *p = 5;
  return *p;
@}
@end smallexample

The assignment @code{*p = 5} may be a definition of @code{a} or
@code{b}.  If we cannot determine statically where @code{p} is
pointing to at the time of the store operation, we create virtual
definitions to mark that statement as a potential definition site for
@code{a} and @code{b}.  Memory loads are similarly marked with virtual
use operands.  Virtual operands are shown in tree dumps right before
the statement that contains them.  To request a tree dump with virtual
operands, use the @code{-vops} option to @code{-fdump-tree}:

@smallexample
@{
  int a, b, *p;

  if (...)
    p = &a;
  else
    p = &b;
  # a = VDEF <a>
  # b = VDEF <b>
  *p = 5;

  # VUSE <a>
  # VUSE <b>
  return *p;
@}
@end smallexample

Notice that @code{VDEF} operands have two copies of the referenced
variable.  This indicates that this is not a killing definition of
that variable.  In this case we refer to it as a @dfn{may definition}
or @dfn{aliased store}.  The presence of the second copy of the
variable in the @code{VDEF} operand will become important when the
function is converted into SSA form.  This will be used to link all
the non-killing definitions to prevent optimizations from making
incorrect assumptions about them.

Operands are collected by @file{tree-ssa-operands.c}.  They are stored
inside each statement's annotation and can be accessed with
@code{DEF_OPS}, @code{USE_OPS}, @code{VDEF_OPS} and @code{VUSE_OPS}.
The following are all the accessor macros available to access USE
operands.  To access all the other operand arrays, just change the
name accordingly:

@ftable @code
@item USE_OPS(ANN)
Returns the array of operands used by the statement with annotation
@code{ANN}.

@item STMT_USE_OPS(STMT)
Alternate version of USE_OPS that takes the statement as input.

@item NUM_USES(OPS)
Return the number of USE operands in array OPS.

@item USE_OP_PTR(OPS, I)
Return a pointer to the Ith operand in array OPS.

@item USE_OP(OPS, I)
Return the Ith operand in array OPS.
@end ftable

The following function shows how to print all the operands of a given
statement:

@example
void
print_ops (tree stmt)
@{
  vuse_optype vuses;
  vdef_optype vdefs;
  def_optype defs;
  use_optype uses;
  stmt_ann_t ann;
  size_t i;

  get_stmt_operands (stmt);
  ann = stmt_ann (stmt);
  
  defs = DEF_OPS (ann);
  for (i = 0; i < NUM_DEFS (defs); i++)
    print_generic_expr (stderr, DEF_OP (defs, i), 0);

  uses = USE_OPS (ann);
  for (i = 0; i < NUM_USES (uses); i++)
    print_generic_expr (stderr, USE_OP (uses, i), 0);
  
  vdefs = VDEF_OPS (ann);
  for (i = 0; i < NUM_VDEFS (vdefs); i++)
    print_generic_expr (stderr, VDEF_OP (vdefs, i), 0);
  
  vuses = VUSE_OPS (ann);
  for (i = 0; i < NUM_VUSES (vuses); i++)
    print_generic_expr (stderr, VUSE_OP (vuses, i), 0);
@}
@end example

To collect the operands, you first need to call
@code{get_stmt_operands}.  Since that is a potentially expensive
operation, statements are only scanned if they have been marked
modified by a call to @code{modify_stmt}.  So, if your pass replaces
operands in a statement, make sure to call @code{modify_stmt}.


@node SSA
@section Static Single Assignment
@cindex SSA
@cindex static single assignment

Most of the tree optimizers rely on the data flow information provided
by the Static Single Assignment (SSA) form.  We implement the SSA form
as described in @cite{R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and
K. Zadeck. Efficiently Computing Static Single Assignment Form and the
Control Dependence Graph. ACM Transactions on Programming Languages
and Systems, 13(4):451-490, October 1991}.

The SSA form is based on the premise that program variables are
assigned in exactly one location in the program.  Multiple assignments
to the same variable create new versions of that variable.  Naturally,
actual programs are seldom in SSA form initially because variables
tend to be assigned multiple times.  The compiler modifies the program
representation so that every time a variable is assigned in the code,
a new version of the variable is created.  Different versions of the
same variable are distinguished by subscripting the variable name with
its version number.  Variables used in the right-hand side of
expressions are renamed so that their version number matches that of
the most recent assignment.

We represent variable versions using @code{SSA_NAME} nodes.  The
renaming process in @file{tree-ssa.c} wraps every real and
virtual operand with an @code{SSA_NAME} node which contains
the version number and the statement that created the
@code{SSA_NAME}.  Only definitions and virtual definitions may
create new @code{SSA_NAME} nodes.

Sometimes, flow of control makes it impossible to determine what is the
most recent version of a variable.  In these cases, the compiler
inserts an artificial definition for that variable called
@dfn{PHI function} or @dfn{PHI node}.  This new definition merges
all the incoming versions of the variable to create a new name
for it.  For instance,

@example
if (...)
  a_1 = 5;
else if (...)
  a_2 = 2;
else
  a_3 = 13;

# a_4 = PHI <a_1, a_2, a_3>
return a_4;
@end example

Since it is not possible to determine which of the three branches
will be taken at runtime, we don't know which of @code{a_1},
@code{a_2} or @code{a_3} to use at the return statement.  So, the
SSA renamer creates a new version @code{a_4} which is assigned
the result of ``merging'' @code{a_1}, @code{a_2} and @code{a_3}.
Hence, PHI nodes mean ``one of these operands.  I don't know
which''.

The following macros can be used to examine PHI nodes

@ftable @code
@item	PHI_RESULT(PHI)
Returns the SSA_NAME created by this PHI node (i.e., its LHS).

@item	PHI_NUM_ARGS(PHI)
Returns the number of arguments in PHI.  This number is exactly the
number of incoming edges to the basic block holding PHI.

@item	PHI_ARG_ELT(PHI, I)
Returns a tuple representing the Ith argument of PHI.  Each element of
this tuple contains an SSA_NAME and the incoming edge through which
this name flows.

@item	PHI_ARG_EDGE(PHI, I)
Returns the incoming edge for the Ith argument of PHI.

@item	PHI_ARG_DEF(PHI, I)
Returns the SSA_NAME for the Ith argument of PHI.
@end ftable


@subsection Preserving the SSA form
@findex vars_to_rename
@cindex preserving SSA form
Some optimization passes make changes to the function that
invalidate the SSA property.  This can happen when a pass has
added new variables or changed the program so that variables that
were previously aliased aren't anymore.

Whenever something like this happens, the affected variables must
be renamed into SSA form again.  To do this, you should mark the
new variables in the global bitmap @code{vars_to_rename}.  Once
your pass has finished, the pass manager will invoke the SSA
renamer to put the program into SSA once more.

@subsection Examining SSA_NAME nodes
@cindex examining SSA_NAMEs
The following macros can be used to examine SSA_NAME nodes

@ftable @code
@item SSA_NAME_DEF_STMT
Returns the statement S that creates the given SSA_NAME.  If S is
an empty statement (@code{IS_EMPTY_STMT}), it means that the
first reference to this variable is a USE or a VUSE.

@item SSA_NAME_VERSION
Return the version number of this SSA_NAME object.
@end ftable

@subsection Walking use-def chains
@findex @code{walk_use_def_chains (VAR, FN, DATA)}
Walks use-def chains starting at the SSA variable @code{VAR}.  Calls
function @code{FN} at each reaching definition found.  @code{FN} takes
three arguments: @code{VAR}, its defining statement (@code{DEF_STMT})
and a generic pointer to whatever state information that @code{FN} may want
to maintain (@code{DATA}).

Note, that if @code{DEF_STMT} is a @code{PHI} node, the semantics are
slightly different. For each argument @code{ARG} of the PHI node, this
function will:

@enumerate
@item	Walk the use-def chains for @code{ARG}.
@item	Call @code{FN (ARG, PHI, DATA)}.
@end enumerate

Note how the first argument to @code{FN} is no longer the original
variable @code{VAR}, but the PHI argument currently being examined.
If @code{FN} wants to get at @code{VAR}, it should call
@code{PHI_RESULT} (PHI).

@subsection Walking the dominator tree
@cindex walk dominator tree

In progress.


@node Alias analysis
@section Alias analysis
@cindex alias
@cindex flow-sensitive alias analysis
@cindex flow-insensitive alias analysis

Alias analysis proceeds in 3 main phases:

@enumerate
@item	Points-to and escape analysis.

This phase walks the use-def chains in the SSA web looking for
three things:

	@itemize @bullet
	@item	Assignments of the form @code{P_i = &VAR}
	@item	Assignments of the form P_i = malloc()
	@item	Pointers and ADDR_EXPR that escape the current function.
	@end itemize

The concept of 'escaping' is the same one used in the Java world.
When a pointer or an ADDR_EXPR escapes, it means that it has been
exposed outside of the current function.  So, assignment to
global variables, function arguments and returning a pointer are
all escape sites.

This is where we are currently limited.  Since not everything is
renamed into SSA, we lose track of escape properties when a
pointer is stashed inside a field in a structure, for instance.
In those cases, we are assuming that the pointer does escape.

We use escape analysis to determine whether a variable is
call-clobbered.  Simply put, if an ADDR_EXPR escapes, then the
variable is call-clobbered.  If a pointer P_i escapes, then all
the variables pointed-to by P_i (and its memory tag) also escape.

@item	Compute flow-sensitive aliases

We have two classes of memory tags.  Memory tags associated with
the pointed-to data type of the pointers in the program.  These
tags are called "type memory tag" (TMT).  The other class are
those associated with SSA_NAMEs, called "name memory tag" (NMT).
The basic idea is that when adding operands for an INDIRECT_REF
*P_i, we will first check whether P_i has a name tag, if it does
we use it, because that will have more precise aliasing
information.  Otherwise, we use the standard type tag.

In this phase, we go through all the pointers we found in
points-to analysis and create alias sets for the name memory tags
associated with each pointer P_i.  If P_i escapes, we mark
call-clobbered the variables it points to and its tag.


@item	Compute flow-insensitive aliases

This pass will compare the alias set of every type memory tag and
every addressable variable found in the program.  Given a type
memory tag TMT and an addressable variable V.  If the alias sets
of TMT and V conflict (as computed by may_alias_p), then V is
marked as an alias tag and added to the alias set of TMT.
@end enumerate

For instance, consider the following function:

@example
foo (int i)
@{
  int *p, *q, a, b;

  if (i > 10)
    p = &a;
  else
    q = &b;

  *p = 3;
  *q = 5;
  a = b + 2;
  return *p;
@}
@end example

After aliasing analysis has finished, the type memory tag for
pointer 'p' will have two aliases, namely variables 'a' and 'b'.
Every time pointer 'p' is dereferenced, we want to mark the
operation as a potential reference to 'a' and 'b'.

@example
foo (int i)
@{
  int *p, a, b;

  if (i_2 > 10)
    p_4 = &a;
  else
    p_6 = &b;
  # p_1 = PHI <p_4(1), p_6(2)>;

  # a_7 = VDEF <a_3>;
  # b_8 = VDEF <b_5>;
  *p_1 = 3;

  # a_9 = VDEF <a_7>
  # VUSE <b_8>
  a_9 = b_8 + 2;

  # VUSE <a_9>;
  # VUSE <b_8>;
  return *p_1;
@}
@end example

In certain cases, the list of may aliases for a pointer may grow
too large.  This may cause an explosion in the number of virtual
operands inserted in the code.  Resulting in increased memory
consumption and compilation time.

When the number of virtual operands needed to represent aliased
loads and stores grows too large (configurable with @option{--param
max-aliased-vops}), alias sets are grouped to avoid severe
compile-time slow downs and memory consumption.  The alias
grouping heuristic proceeds as follows:

@enumerate
@item	Sort the list of pointers in decreasing number of contributed
	virtual operands.

@item	Take the first pointer from the list and reverse the role
	of the memory tag and its aliases.  Usually, whenever an
	aliased variable Vi is found to alias with a memory tag
	T, we add Vi to the may-aliases set for T.  Meaning that
	after alias analysis, we will have:

		may-aliases(T) = @{ V1, V2, V3, ..., Vn @}

	This means that every statement that references T, will get 'n'
	virtual operands for each of the Vi tags.  But, when alias
	grouping is enabled, we make T an alias tag and add it to the
	alias set of all the Vi variables:

		may-aliases(V1) = @{ T @}
		may-aliases(V2) = @{ T @}
		...
		may-aliases(Vn) = @{ T @}

	This has two effects: (a) statements referencing T will only get
	a single virtual operand, and, (b) all the variables Vi will now
	appear to alias each other.  So, we lose alias precision to
	improve compile time.  But, in theory, a program with such a high
	level of aliasing should not be very optimizable in the first
	place.

@item	Since variables may be in the alias set of more than one
	memory tag, the grouping done in step (2) needs to be extended
	to all the memory tags that have a non-empty intersection with
	the may-aliases set of tag T.  For instance, if we originally
	had these may-aliases sets:

		may-aliases(T) = @{ V1, V2, V3 @}
		may-aliases(R) = @{ V2, V4 @}

	In step (2) we would have reverted the aliases for T as:

		may-aliases(V1) = @{ T @}
		may-aliases(V2) = @{ T @}
		may-aliases(V3) = @{ T @}

	But note that now V2 is no longer aliased with R.  We could
	add R to may-aliases(V2), but we are in the process of
	grouping aliases to reduce virtual operands so what we do is
	add V4 to the grouping to obtain:

		may-aliases(V1) = @{ T @}
		may-aliases(V2) = @{ T @}
		may-aliases(V3) = @{ T @}
		may-aliases(V4) = @{ T @}

@item	If the total number of virtual operands due to aliasing is
	still above the threshold set by max-alias-vops, go back to (2).
@end enumerate
