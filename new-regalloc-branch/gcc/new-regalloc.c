/* New register allocator 
   Copyright (C) 2001 Free Software Foundation, Inc.
   Contributed by Daniel Berlin <dberlin@redhat.com>

This file is part of GNU CC.

GNU CC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2, or (at your option)
any later version.

GNU CC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with GNU CC; see the file COPYING.  If not, write to
the Free Software Foundation, 59 Temple Place - Suite 330,
Boston, MA 02111-1307, USA.  

*/

/* TODO: 
   1. Rip spill code insertion from reload, put it here instead, so we
   can iterate.
   2. Allow spill code heuristics to be plugged in. (50% done)
   3. Large amounts of tuning. 
   4. Use right size for various bitmaps. We don't actually need
   max_reg_num(), just max_reg_num - FIRST_PSEUDO_REGISTER.

*/
/* STATUS:
   When we don't spill, we have perfect allocation. Reload doesn't
   change anything. Thus, we are satistfying all constraints required,
   at least on PowerPC.

*/
/* The algorithm used is iterated register coalescing by George and
   Appel.
   The divergence from the paper is twofold. First, in the use of
   register freedom to  allow the graph to have registers with
   different/multiple classes, and properly allocate for them, on it.
   This adds 7 or 8 lines of code, total. no joke.
   
   The second is in edge weighting, which allows us to properly
   express the interference when some pseudos really need more hard
   regs than other pseudos.  The other way to do this is with
   multi-graphs (IE treat a pseudo as multiple on the graph), but this
   way is equivalent, and easier to implement. 
   
   We assume consecutive regs are necessary to express a pseudo that
   requires multiple hard registers. 
   We try to take into account the register preference, even with
   multiple hard regs needed.
   In fact, this is what find_reg_given_constraints does. Given a set
   of okay registers, find the best one, given all the constraints and
   preferences(IE allocation order, whether the register is okay for
   this mode, pseudo needing multiple hard regs, etc)
   We do this by finding the most preferred register (using the
   allocation order) that has the right number of usable registers in either
   direction.
   We then treat the register as if it had multiple colors (by
   excluding those extra consecutive registers when coloring it's
   neighbors).

   This register allocator needs to be able to generate the spill
   code, and handles register allocation for temporaries generated by
   that spill code. It does this by redoing register allocation
   whenever we spill.
   Each round of register allocation is linear, so this is not a drag
   at all.

   We rarely spill. It takes a serious amount of work to make us spill
   on an architecture with a reasonable amount of registers. Even on
   x86, it takes a *lot* to make us spill.
   
   Advantages over old allocator:
   Eliminates regmove - We coalesce more moves anyway.
   Eliminates spill allocation, just need to insert spill code, and
   rerun register allocation.
   Performs a *ton* better. On my powerpc, I haven't hit a spill in
   compiling gcc's associated libraries (libiberty, libstdc++-v3,
   etc). We used to spill multiple times per procedure for each file.
   On x86, with it's severely limited register classes
   Easy to understand - you can look at the paper, and actually
   understand it,  and look at the pseudocode given, and match it up
   with the code here. 
   Easy to maintain - the algorithms are elegant, and simple. There is
   no machine-specific cruftiness here. Liveness analysis is taken
   care of by the dataflow analysis module.
   Easy to debug - The code necessary to print out the sets being used
   in an easy to read form is included, as is enough debugging info to
   tell you what is going on at a high level, and a low level (you can
   control the level of spew) 


   Possible Improvements (In order of bang for the buck ):

   (Very easy, Significant improvement in running time of register
   allocator )
   It is safe (IE won't increase the number of spills ) to keep all
   coalescings performed before the first spill  node is removed from
   the graph. Since many coalescings occur before 
   the first spill, the graph used in subsequent rounds would be
   smaller, and the algorithm would go significantly faster.
   
   (Easy, significant improvement in spill selection )
   Variables with constant values can be spilled very cheaply (since
   no store is necessary, and we can recompute the value at any point
   if needed ). We should use this information in choosing spills,
   since the constant propagation pass gives it to us (right?).
   
   (Medium difficulty, unless you are an optimization person, in which
   case this is probably easy. No idea on improvement)
   Try out optimistic register coalescing. This gives you all the
   building blocks to do it, it wouldn't take very long at all to do
   this.
   
   We could integrate copy/const propagation into this register
   allocator if we wanted to.
   
*/
#include "new-regalloc.h"
#include "regs.h"
#include "hard-reg-set.h"
#include "rtl.h"
#include "basic-block.h"
#include "function.h"
#include "output.h"
#include "ggc.h"
#include "df.h"
#include "varray.h"
#include "hashtab.h"
#include "glimits.h"
/* Hashed set. */
typedef htab_t hset;
typedef htab_trav hset_trav;

static hashval_t hset_hash PARAMS((const void *));
static int hset_equal PARAMS((const void *, const void *));
static void hset_free PARAMS((void *));
static hset hset_new PARAMS((void));
static void hset_delete PARAMS((hset));
static void hset_insert PARAMS((hset, void *));
static void hset_remove PARAMS((hset, void *));
static int hset_member PARAMS((hset, void *));
static int hset_union_1 PARAMS((void **, void *));
static void hset_union PARAMS((hset, hset));
static int hset_empty PARAMS((hset));
static void hset_traverse PARAMS((hset, hset_trav, void *));
static int hset_subtract_1 PARAMS((void **, void *));
static void hset_subtract PARAMS((hset, hset));
static int hset_intersect_1 PARAMS((void **, void *));
static void hset_intersect PARAMS((hset, hset));
static int hset_print_intset_1 PARAMS((void **, void *));
static void hset_print_intset PARAMS((FILE *, hset, const char *));
void debug_integer_hset PARAMS((hset));

/* This macro exists because there are places where we'd have to
   create large structs just to pass all the information we need. */
#define HSET_TRAVERSAL(set, counter, entry, code) \
for (counter = 0; counter < set->size; counter++) \
if (set->entries[counter] != (void *) 0 && \
    set->entries[counter] != (void *) 1) \
{ \
entry = set->entries[counter]; \
code; \
} \


/* IRC routines */
static int edge_weight PARAMS((unsigned int, unsigned int));
static void add_edge PARAMS((unsigned int, unsigned int));
static void add_move PARAMS((unsigned int, rtx));
static int is_reg_candidate PARAMS((unsigned int));
static int is_candidate_move PARAMS((rtx, sbitmap));
static void make_worklist PARAMS((sbitmap));
static hset node_moves PARAMS((unsigned int));
static int move_related PARAMS((unsigned int));
static int varray_contains PARAMS((varray_type, unsigned int));
static sbitmap adjacent PARAMS((unsigned int));
static int enable_moves_1 PARAMS((void **, void *));
static void enable_moves PARAMS((unsigned int));
static void decrement_degree PARAMS((unsigned int, unsigned int));
static void simplify PARAMS((void));
static void coalesce PARAMS((void));
static void freeze_moves PARAMS((unsigned int));
static void freeze PARAMS((void));
float (*heuristic) PARAMS((unsigned int));
static float default_heuristic PARAMS((unsigned int));
static void select_spill PARAMS((void));
static void assign_regs PARAMS((void));
static void rewrite_program PARAMS((void));
static unsigned int get_alias PARAMS((unsigned int));
static rtx find_costliest_move PARAMS((hset));
static void add_worklist PARAMS((unsigned int));
static int OK PARAMS((unsigned int, unsigned int));
static int conservative PARAMS((unsigned int, unsigned int));
static void combine PARAMS((unsigned int, unsigned int));
static int reg_freedom PARAMS((unsigned int));

void init_new_regalloc PARAMS((void));
void perform_new_regalloc_init PARAMS((void));
void perform_new_regalloc PARAMS((void));
void finish_new_regalloc PARAMS((void));


/* The naming convention of the routines/variables follows the paper,
   I'll clean it up later */ 
typedef struct i_g_node
{
  hset moveList;
  sbitmap adjList; 
  unsigned int alias;
  int degree;
} *ig_node;

static ig_node ig_node_new PARAMS((void));
static void print_ig_node PARAMS((FILE *, void *));
void debug_ig_node PARAMS((void *));

/* return a new interference graph node */
static
ig_node ig_node_new ()
{
  ig_node retval;
  retval = ggc_alloc_cleared(sizeof(struct i_g_node));
  retval->adjList = sbitmap_alloc(max_reg_num());
  sbitmap_zero(retval->adjList);
  
  retval->moveList = hset_new();
  return retval;
  
}

static 
void print_ig_node (file, node)
     FILE *file;
     void *node;
{
  ig_node theNode = (ig_node) node;
  fprintf(file, "{ Degree:%d, Alias:%d, ", theNode->degree,
	  theNode->alias);
  //  hset_print_intset(file, theNode->adjList, "adjList");
  fprintf(file, "}\n");
  
}

void debug_ig_node(node)
     void *node;
{
  print_ig_node(stderr, node);
}

/* Hash function for hashed sets */  
static
hashval_t hset_hash(a)
     const void *a;
{
  return (unsigned int) a;
}

/* Hashed set element equality */
static
int hset_equal (a, b)
     const void *a;
     const void *b;
{
  return a==b;
}

/* Hashed set element free.
 We have nothing that isn't a pointer, or needs freeing, so we no-op
 this routine. */
static
void hset_free (a)
     void *a ATTRIBUTE_UNUSED;
     
{
 
}

/* Create a new hashed set */
static 
hset hset_new()
{
    return htab_create(119,hset_hash, hset_equal, hset_free);
}

/* Delete an existing hashed set */
static
void hset_delete(set)
     hset set;
{
  htab_delete(set);
}

/* Insert an element into the hashed set */
static 
void hset_insert(set, elt)
     hset set;
     void *elt;
     
{
  *(htab_find_slot(set,elt, INSERT))=elt;
}

/* Remove an element from the hashed set */
static
void hset_remove(set, elt)
     hset set;
     void *elt;
{
  htab_remove_elt(set, elt);
}

/* Determine if elt is a member of hashed set set */
static
int hset_member(set, elt)
     hset set;
     void *elt;
{
  return htab_find(set, elt) != 0;
}

/* Stupid value passing structure */
struct twosets
{
  hset thisset;
  hset otherset;
};

/* Union helper routine */
static 
int hset_union_1(entry, currset)
     void **entry;
     void *currset;
{
  hset_insert((hset) currset, *entry);
  return 1;
}

/* Hashed set union */
static 
void hset_union(set1, set2)
     hset set1;
     hset set2;
{
  htab_traverse(set2, hset_union_1, set1);
}

/* Hashed set traversal */
static
void hset_traverse(set, func, data)
     hset set;
     hset_trav func;
     void *data;
{
  htab_traverse(set,func,data);
}
/* Return 1 if hashed set is empty */
static
int hset_empty(set)
     hset set;
{
  return htab_elements(set) == 0;
}

/* Subtraction helper routine */
static
int hset_subtract_1(entry, twosets)
     void **entry;
     void *twosets;
{
  if (hset_member(((struct twosets *) twosets)->otherset, *entry))
    {
      hset_remove(((struct twosets *) twosets)->thisset, *entry);
    }
  return 1;
}

/* set1 <- set1 - set2 */
static 
void hset_subtract(set1, set2)
     hset set1;
     hset set2;
{
  struct twosets *newtwo;
  newtwo = xmalloc(sizeof(struct twosets));
  newtwo->thisset = set1;
  newtwo->otherset = set2;
  htab_traverse(set1, hset_subtract_1, newtwo);
  free(newtwo);
}

/* Intersection helper routine */
static 
int hset_intersect_1(entry, twosets)
     void **entry;
     void *twosets;
{
  if (!hset_member(((struct twosets *) twosets)->otherset, *entry))
    {
      hset_remove(((struct twosets *) twosets)->thisset, *entry);
    }
  return 1;
}

/* set1 <- set1 INTERSECT set2 */
static
void hset_intersect(set1, set2)
     hset set1;
     hset set2;
{
  struct twosets *newtwo;
  newtwo = xmalloc(sizeof(struct twosets));
  newtwo->thisset = set1;
  newtwo->otherset = set2;
  
  htab_traverse(set1, hset_intersect_1, newtwo);
  free(newtwo);
  
}

/* Print out an integer set helper*/
static
int hset_print_intset_1(entry, file)
     void **entry;
     void *file;
     
{
  fprintf((FILE *)file, "%d ", (unsigned int)*entry);
  return 1;
}

/* Print out an integer set */
static
void hset_print_intset(file, set, name)
     FILE *file;
     hset set;
     const char *name;
     
{
  fprintf(file, "Integer set \"%s\":{", name);
  htab_traverse(set, hset_print_intset_1, file);
  fprintf(file, "}\n");
}

/* debugger function to print out integer set */
void
debug_integer_hset(set)
     hset set;
{
  hset_print_intset(stderr, set, "");
}

/* Precolored nodes */
sbitmap precolored;

/* Rest are from paper */
sbitmap simplifyWorklist;
sbitmap freezeWorklist;
sbitmap spillWorklist;
sbitmap spilledNodes;
sbitmap coalescedNodes;
hset coalescedMoves;
hset constrainedMoves;
hset frozenMoves;
hset worklistMoves;
hset activeMoves;

/* Map register numbers to interference graph nodes */
ig_node *regInfo;
int *colors;


varray_type selectStack;

struct df * dataflowAnalyser;
/*
  0 = no spew
  1 = not much spew
  2 = lots of spew 
  Amazingly useful for debugging this allocator.
*/
static int debug_new_regalloc=0;

/* Determine if regno is a candidate for this pass of register
   allocation. */
static
int is_reg_candidate (regno)
     unsigned int regno;
{
  return (DF_REGNO_FIRST_DEF(dataflowAnalyser, regno) != 0);
}

/* Determine if a given rtx is a candidate move */
static
int is_candidate_move(insn, set)
     rtx insn;
     sbitmap set;
     
{
  if (!(GET_CODE(insn) == INSN))
    return 0;
  
  if (GET_CODE(PATTERN(insn)) == SET)
    {
      if (GET_CODE(SET_SRC(PATTERN(insn))) == REG)
	if (GET_CODE(SET_DEST(PATTERN(insn))) == REG)
	  if (TEST_BIT(set, REGNO(SET_SRC(PATTERN(insn)))) &&
	      TEST_BIT(set, REGNO(SET_DEST(PATTERN(insn)))))
	    return 1;
    }
  return 0;
}

/* Determine the edge weight from reg v1 to reg v2 */
static
int edge_weight(v1, v2)
     unsigned int v1;
     unsigned int v2;
{
  int w1 = (TEST_BIT(precolored, v1) ? 0 : CLASS_MAX_NREGS(reg_preferred_class(v1), PSEUDO_REGNO_MODE(v1)));
  int w2 = (TEST_BIT(precolored, v2) ? 0 : CLASS_MAX_NREGS(reg_preferred_class(v2), PSEUDO_REGNO_MODE(v2)));
  
  switch (w1 + w2)
    {
    case 0:
    case 1:
    case 2:
      return 1;
    case 3:
      return 2;
    case 4:
      /* FIXME:Test for unaligned pairs */
      return 2;
    default:
      abort();
    }
  return -1; /* pacify compiler */
}

  
/* Add an interference edge from reg v1 to reg v2 */
static 
void add_edge(v1, v2)
     unsigned int v1;
     unsigned int v2;
{
  if ((!TEST_BIT(regInfo[v1]->adjList, v2)) &&  (v1 != v2))
    {
      /* 
	 Precolored nodes that interfere with each other are
	 irrelevant, we don't color them anyway (since they are
	 precolored */
      
      if (!TEST_BIT(precolored, v1))
	{
	  /* Add v2 to v1's adjacency list. */
	  SET_BIT(regInfo[v1]->adjList, v2);
	  regInfo[v1]->degree += edge_weight(v1, v2);
	}
      if (!TEST_BIT(precolored, v2))
	{
	  /* Add v1 to v2's adjacency list. */
	  SET_BIT(regInfo[v2]->adjList, v1);
	  regInfo[v2]->degree += edge_weight(v2, v1);
	}
    }
  
}

/* Add a move to the regno's movelist */

static 
void add_move(regno, insn)
     unsigned int regno;
     rtx insn;
{
  hset_insert(regInfo[regno]->moveList, (void *)insn);
}

/* node_moves(n) = moveList[n] INTERSECT (activeMoves U worklistMoves) */
static
hset node_moves(regNum)
     unsigned int regNum;
{
  hset result = hset_new();
  hset_union(result, activeMoves);
  hset_union(result, worklistMoves);
  hset_intersect(result, regInfo[regNum]->moveList);
  return result;
  
}

/* Determine if regNum is involved in a move */
static 
int move_related(regNum)
     unsigned int regNum;
{
  int retval;
  hset moves;
  
  moves = node_moves(regNum);
  retval = ! hset_empty(moves);
  hset_delete(moves);
  
  return retval;
}

/* Determine the size of a hard register set.
   If this gets to be a bottleneck, cache it, it won't change. */
#define HARD_REG_SET_SIZE(SET, VAR) \
do { \
int i; \
for (i=0; i < FIRST_PSEUDO_REGISTER; i++) \
   if (TEST_HARD_REG_BIT(SET, i)) \
      VAR++; \
} while (0)

/* Determine the register freedom of regNum.
   Register freedom is what lets us allocate all the register types in
   one pass. It determines the number of registers in the register set
   this regNum prefers.  This assumes that registers only have
   preferences that are valid classes, which they do, AFAIK. */
static
int reg_freedom(regNum)
     unsigned int regNum;
{
  HARD_REG_SET temp;
  unsigned int j=0;
  
  COPY_HARD_REG_SET(temp, reg_class_contents[reg_preferred_class(regNum)]);
  IOR_HARD_REG_SET(temp, reg_class_contents[reg_alternate_class(regNum)]);
  
  HARD_REG_SET_SIZE(temp, j);
  return j;
  
}
/* Make the worklists from the inital set of candidates */
static
void make_worklist(set)
     sbitmap set;
{

  unsigned int entry;
  
  EXECUTE_IF_SET_IN_SBITMAP(set, 0, entry,
  {
    unsigned int regNum = entry;
    /* Remove it from the initial set */
    RESET_BIT(set, entry);
    /* If it's of significant degree, place it on the spillWorklist */
    if (regInfo[regNum]->degree >= reg_freedom(regNum))
      {
	if (debug_new_regalloc > 1)
	  fprintf(stderr, "Inserting %d onto spillWorklist\n",
		  regNum);
	SET_BIT(spillWorklist, regNum);
      }
    /* If it's move related, put it on the freeze worklist */
    else if (move_related(regNum))
      {
	if (debug_new_regalloc > 1)
	  fprintf(stderr, "Inserting %d onto freezeWorklist\n",
		  regNum);
	SET_BIT(freezeWorklist, regNum);
      }
    /* Otherwise, put it on the simplify worklist */
    else
      {
	if (debug_new_regalloc > 1)
	  fprintf(stderr, "Inserting %d onto simplifyWorklist\n",
		  regNum);
	SET_BIT(simplifyWorklist, regNum);
      }
  });
  
			    
}
/* Returns 1 if the varray contains the register regNum */
static
int varray_contains(array, regNum)
     varray_type array;
     unsigned int regNum;
{
  unsigned int i;
  for (i=0; i < VARRAY_SIZE(array); i++)
    if (VARRAY_UINT(array, i) == regNum)
      return 1;
  return 0;
  
}

/* adjacent(n) = adjList[n] - (selectStack U coalescedMoves) */
static
sbitmap adjacent(regNum)
     unsigned int regNum;
{
  sbitmap result;
  unsigned int entry;
  
  result = sbitmap_alloc(max_reg_num());
  sbitmap_zero(result);
  sbitmap_a_or_b(result, result, regInfo[regNum]->adjList);
  sbitmap_difference(result, result, coalescedNodes);
  EXECUTE_IF_SET_IN_SBITMAP(result, 0, entry, 
  {
    if (varray_contains(selectStack, entry))
      {
	RESET_BIT(result, entry);
      }
  });

  return result;
  
}
/* enable_moves helper function */
static
int enable_moves_1(entry, set)
     void **entry;
     void *set ATTRIBUTE_UNUSED;
{
  /* If it is currently an active move, put it on the worklist for
     moves */
  if (hset_member(activeMoves, *entry))
    {
      hset_remove(activeMoves, *entry);
      hset_insert(worklistMoves, *entry);
    }
  return 1;
}

/* Enable moves for register number regNum.
   Takes the moves for this register, and puts them on the worklist to
   be processed */
static
void enable_moves(regNum)
     unsigned int regNum;
{
  hset moves;
  moves = node_moves(regNum);
  hset_traverse(moves, enable_moves_1, moves);
  hset_delete(moves);
}

/* Decrement the degree of the register, handle possible effect (IE it
   might move from significant degree to insignificant degree ) */
static
void decrement_degree(regNum, from)
     unsigned int regNum;
     unsigned int from;
     
{
  int w = edge_weight(regNum, from);
  int d = regInfo[regNum]->degree;
  sbitmap adj;
  unsigned int entry;
  
  /* If it was of significant degree before, and isn't now, we need to
     enable its moves, and move it off the spillWorklist */
  regInfo[regNum]->degree -= w;
  
  if (regInfo[regNum]->degree < reg_freedom(regNum) && d >= reg_freedom(regNum))
    {
      /* Make all moves involving variable ready for coalescing */
      enable_moves(regNum);
      adj = adjacent(regNum);
      EXECUTE_IF_SET_IN_SBITMAP(adj, 0, entry, 
      {
	enable_moves(entry);
      });
      sbitmap_free(adj);
      
      /* This variable is now low-degree... treat him that way. */

      RESET_BIT(spillWorklist, regNum);
      
      if (move_related(regNum))
	{
	  SET_BIT(freezeWorklist, regNum);
	}
      else
	{
	  SET_BIT(simplifyWorklist, regNum);
	}
    }
}

/* simplify the graph */
static
void simplify()
{
  unsigned int currReg;
  unsigned int entry;
  
  sbitmap adj;
  /* Pick the first thing on the worklist */
  currReg = (unsigned int) sbitmap_first_set_bit(simplifyWorklist);
  if (debug_new_regalloc > 1)
    fprintf(stderr, "picked reg %d in simplify\n", currReg);
  /* Remove it */
  RESET_BIT(simplifyWorklist, currReg);
  
  /* Push it onto the selectStack to be colored */
  if (VARRAY_ACTIVE_SIZE(selectStack) + 1 > VARRAY_SIZE(selectStack))
    VARRAY_GROW(selectStack, VARRAY_SIZE(selectStack) * 2);
  
  VARRAY_PUSH_UINT(selectStack, currReg);
  
  adj = adjacent(currReg);
  /* From new register allocator paper */
  if (regInfo[currReg]->degree > reg_freedom(currReg))
    EXECUTE_IF_SET_IN_SBITMAP(adj, 0, entry,
    {
      enable_moves(entry);
    });

  /* Now that we've removed it, everything adjacent to it gets
     decremented in degree */
  EXECUTE_IF_SET_IN_SBITMAP(adj, 0, entry, 
  {
    if (debug_new_regalloc > 1)
      fprintf(stderr, "Decrementing degree of %d\n", entry);
    decrement_degree(entry, currReg);
  });

  sbitmap_free(adj);
  
}

/* If regNum was coalesced, find out where it went, recursively if
   necessary */
static
unsigned int get_alias(regNum)
     unsigned int regNum;
{
  unsigned int result;
  if (TEST_BIT(coalescedNodes, regNum))
    {
      result = get_alias(regInfo[regNum]->alias);
    }
  else
    {
      result = regNum;
    }
  return result;
}

/* Find the move with the maximum cost */
static 
rtx find_costliest_move(moves)
     hset moves;
{
  unsigned int i;
  void *entry;
  float maxCost =(float) -1.0;
  rtx costliest = NULL_RTX;

  HSET_TRAVERSAL(moves, i, entry, 
  {
    /* XXX: Do we have a better heuristic, or real move costs? I
       didn't look very hard at all. 
       Right now we assume move cost is based on loop depth, since the
       move gets done every iteration .
    */
    float cost;
    cost = BLOCK_FOR_INSN((rtx)entry)->loop_depth * REGISTER_MOVE_COST(GET_MODE((rtx) entry), REGNO_REG_CLASS(REGNO(SET_DEST(PATTERN((rtx) entry)))), REGNO_REG_CLASS(REGNO(SET_SRC(PATTERN((rtx) entry)))));
    if (cost > maxCost)
      {
	maxCost = cost;
	costliest = entry;
      }
  });
  
  return costliest;
  
}
/* Add_worklist takes a register, and if it's not move related, and it
   is of insignificant degree, puts it on the simplifyWorklist */
static
void add_worklist(regNum)
     unsigned int regNum;
{
  if (!move_related(regNum) && (regInfo[regNum]->degree < reg_freedom(regNum)))
    {
      RESET_BIT(freezeWorklist, regNum);
      SET_BIT(simplifyWorklist, regNum);
      
    }
}

/* OK is the precolored coalescing heuristic */
static
int OK(f, r)
     unsigned int f;
     unsigned int r;
{
  sbitmap adjacentSet;
  unsigned int entry;
  adjacentSet = regInfo[f]->adjList;
  EXECUTE_IF_SET_IN_SBITMAP(adjacentSet, 0, entry,
  {
    if (!(TEST_BIT(precolored, entry) ||
	  /* Double check this one*/
	  TEST_BIT(regInfo[r]->adjList, entry) ||
	  regInfo[entry]->degree < reg_freedom(entry)))
      return 0;
  });
  return 1;
}
/* Called when both rhs and lhs are candidates (not precolored). It
   returns true when the result of coalescing the two would be guaranteed
   to become a node of "insignificant degree", and thus, easy to color.
*/
static 
int conservative(rhs, lhs)
     unsigned int rhs;
     unsigned int lhs;
     
{
  int k = 0;
  int freedom;
  unsigned int entry;
  enum machine_mode lhsmode, rhsmode;
  
  /* Cache visited entries so we don't waste a ton of time */
  sbitmap visited;
  
  visited = sbitmap_alloc(max_reg_num());
  sbitmap_zero(visited);

  /* XXX: If they aren't the same mode, they can't possibly be combined,
     right? */  
  if (HARD_REGISTER_NUM_P(rhs))
    rhsmode = reg_raw_mode[rhs];
  else
    rhsmode = PSEUDO_REGNO_MODE(rhs);
  
  if (HARD_REGISTER_NUM_P(lhs))
    lhsmode = reg_raw_mode[lhs];
  else
    lhsmode = PSEUDO_REGNO_MODE(lhs);
  
  if (!(rhsmode == lhsmode))
    return 0;
  
  freedom = MAX(reg_class_size[reg_preferred_class(rhs)], reg_class_size[reg_preferred_class(rhs)]);
  
  /*FIXME: We really should to keep the class with the node, so we can
    combine the classes when we coalesce */

  /* We don't figure out the full set of neighbors beforehand, as it's
     much slower than prescreening. */
  
  EXECUTE_IF_SET_IN_SBITMAP(regInfo[lhs]->adjList, 0, entry, 
  {
    if (!TEST_BIT(visited, entry))
      {
	SET_BIT(visited, entry);
	if (regInfo[entry]->degree >= reg_freedom(entry))
	  {
	    k += edge_weight(lhs, entry);
	    if (k >= freedom)
	      return 0;
	  }
	
      }
  });
  
  
  EXECUTE_IF_SET_IN_SBITMAP(regInfo[rhs]->adjList, 0, entry,
  {
     if (!TEST_BIT(visited, entry))
      {
	SET_BIT(visited, entry);
	if (regInfo[entry]->degree >= reg_freedom(entry))
	  {
	    k += edge_weight(rhs, entry);
	    
	    if (k >= freedom)
	      return 0;
	  }
	
      }
  });
  sbitmap_free(visited);
  return 1;
  
}

/* combine two nodes into 1, throw it on the right worklists, 
   insert it into the list of coalesced nodes, union the movelists, 
   handle the new neighbors, and setup the alias */
static 
void combine(u, v)
     unsigned int u;
     unsigned int v;
{
  sbitmap adj;
  unsigned int entry;


  if (debug_new_regalloc > 0)
    fprintf(stderr, "Coalescing %d and %d\n", u, v);
  /* Remove v from the worklists, and make it a coalesced node. */
  if (TEST_BIT(freezeWorklist, v))
    {
      RESET_BIT(freezeWorklist, v);
    }
  else
    {
      RESET_BIT(spillWorklist, v);
    }
  /* Throw it on the coalesced node list */
  SET_BIT(coalescedNodes, v);
  /* Set up the alias */
  regInfo[v]->alias = u;
  /* Union the move lists */
  hset_union(regInfo[u]->moveList, regInfo[v]->moveList);
  /* Add our new neighbors, decrement the degree of the old ones */
  adj = adjacent(v);
  EXECUTE_IF_SET_IN_SBITMAP(adj, 0, entry, 
  {
     add_edge(entry, u);
     decrement_degree(entry, v);
  });
  
  sbitmap_free(adj);
  /* The new node might be of significant degree now */
  if (regInfo[u]->degree >= reg_freedom(u) && TEST_BIT(freezeWorklist, u))
    {
      RESET_BIT(freezeWorklist, u);
      SET_BIT(spillWorklist, u);
    }
  
}
/* Coalescing pass */    
static
void coalesce()
{
  rtx as = find_costliest_move(worklistMoves);
  /* as will never be NULL_RTX, if we had no worklistMoves, we
     wouldn't have called coalesce */
  unsigned int lhs = get_alias(REGNO(SET_DEST(PATTERN(as))));
  unsigned int rhs = get_alias(REGNO(SET_SRC(PATTERN(as))));

  /* If the right hand side is precolored, swap the sides */
  if (TEST_BIT(precolored, rhs))
    {
      unsigned int temp;
      temp = lhs;
      lhs = rhs;
      rhs = temp;
    }    
    
  /* Remove it from the worklist, since we are processing it now */
  hset_remove(worklistMoves, as);
  if (lhs == rhs)
    {
      /* Really the same node */
      hset_insert(coalescedMoves, as);
      add_worklist(lhs);
    }
  else
    /* it's constrained if it's precolored, or they interefere */
    {
      if(TEST_BIT(precolored, rhs) || TEST_BIT(regInfo[lhs]->adjList, rhs)) 
	{
	  if (debug_new_regalloc > 1)
	    fprintf(stderr, "Move from %d to %d is constrained\n", lhs,
		    rhs);
	  
	  /* If they interfere, can't coalesce */
	  hset_insert(constrainedMoves, as);
	  add_worklist(lhs);
	  add_worklist(rhs);
	}
      /* Otherwise, check the heuristics, and DTRT */
      else
	{
	  /* We are going to compute the adjacent sets on the fly, so
	     we don't do it here */
	  if ((conservative( lhs, rhs) && ! TEST_BIT(precolored, lhs)) || 
	      (TEST_BIT(precolored, lhs) && OK(rhs, lhs)))
	    {
	      /* Coalesce lhs and rhs conservatively */
	      hset_insert(coalescedMoves, as);
	      combine(lhs, rhs);
	      add_worklist(lhs);
	    }
	  else
	    {
	      /* Can't safely coalesce (yet) */
	      hset_insert(activeMoves, as);
	    }
	}
    }
}
/* Freeze the moves involving the register regNum*/
static
void freeze_moves (regNum)
     unsigned int regNum;
{
  hset moves = node_moves(regNum);
  unsigned int i;
  void *entry;
  

  HSET_TRAVERSAL(moves, i, entry,
  {
    unsigned int v;
    rtx as = (rtx) entry;
    hset nodemoves;
    if (hset_member(activeMoves, (void *)as))
      {
	hset_remove(activeMoves, (void *)as);
      }
    else
      {
	hset_remove(worklistMoves, (void *)as);
      }
    hset_insert(frozenMoves, (void *)as);
    v = REGNO(SET_SRC(PATTERN(as)));
    if (v == regNum)
      v = REGNO(SET_DEST(PATTERN(as)));
    nodemoves = node_moves(v);
    if (hset_empty(nodemoves) && regInfo[v]->degree < reg_freedom(v))
      {
	RESET_BIT(freezeWorklist, v);
	SET_BIT(simplifyWorklist, v);
      }
    hset_delete(nodemoves);
    if (debug_new_regalloc > 1)
      fprintf(stderr, "chug...");
  });
  
  hset_delete(moves);
  
}
/* freeze pass */
static 
void freeze ()
{
  /* Pick a random element from the freeze worklist */
  unsigned int regNum = sbitmap_first_set_bit(freezeWorklist);

  /* And freeze it's moves */
  RESET_BIT(freezeWorklist, regNum);
  SET_BIT(simplifyWorklist, regNum);
  freeze_moves(regNum);
}

/* Heuristic to determine cheapest spill */
static
float default_heuristic (regNum)
     unsigned int regNum;
{
  /*FIXME: probably have a better heuristic */
  return (float) ((float) reg_spill_cost(regNum) / (float) regInfo[regNum]->degree);
}

/* Select the cheapest spill.
 We aren't necessarily spilling anything yet, but */
static
void select_spill()
{
  /* FIXME: Without FLOAT_MAX, a node of degree 1 with a spill cost of
     UINT_MAX will not get picked. What should I be doing here? */
  
  float minCost = UINT_MAX;
  
  unsigned int minNode = 0;
  unsigned int entry;

  EXECUTE_IF_SET_IN_SBITMAP(spillWorklist, 0, entry,
  {
    float h = heuristic(entry);
    if (h < minCost)
      {
	minCost = h;
	minNode = entry;
      }
  });
  
  if (debug_new_regalloc > 1)
    fprintf(stderr, "almost-spilling node %d\n", minNode);
  
  RESET_BIT(spillWorklist, minNode);
  SET_BIT(simplifyWorklist, minNode);
  freeze_moves(minNode);
}
#define FIRST_HARD_REG_BIT_SET(SET, VAR) \
do { \
int i=0; \
for (i=0; i <FIRST_PSEUDO_REGISTER; i++)  \
if (TEST_HARD_REG_BIT(SET, i)) \
{ \
VAR = i; \
break; \
} \
if (i == FIRST_PSEUDO_REGISTER) \
 VAR = -1; \
} while(0)
static 
int x_okay_in_direction(okay, currReg, direction, numRegs)
     HARD_REG_SET okay;
     int currReg;
     int direction;
     int numRegs;
{
  int k;
  
  for (k = 1; k < numRegs; k++)
    if ((currReg+(k*direction) < 0) || !TEST_HARD_REG_BIT(okay, currReg+(k*direction)) || 
	!HARD_REGNO_MODE_OK(currReg+(k*direction), PSEUDO_REGNO_MODE(currReg)))
      return 0;
  return 1;
}
static
int find_reg_given_constraints(okay, currReg)
     HARD_REG_SET okay;
     unsigned int currReg;
{
    int notOK=0;
    int i,k;
    int prefReg=-1;
    int prefRegOrder=INT_MAX;
    int direction=-1;
    unsigned int numRegs=0;
    /* Watch as we attempt to handle the preference, and the
       number of hard registers needed for the pseudo, at the
       same time.
       We find the most preferred reg that lets us allocate the
       right number of hard regs in either direction.
       FIXME: Just use a simple scoring, to get rid of this
       ugliness? Or maybe just ask the arch dependent code to
       tell us what it prefers?
    */
    
    numRegs = 1;
    for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
      if (TEST_HARD_REG_BIT(okay, i))
	{
	  if (HARD_REGNO_MODE_OK(i, PSEUDO_REGNO_MODE(currReg)))
	    {
	      /* May change on each register */
	      numRegs = HARD_REGNO_NREGS(i, PSEUDO_REGNO_MODE(currReg));
	      if (numRegs > 1 && !x_okay_in_direction(okay, i, -1, numRegs-1) && !x_okay_in_direction(okay, i, 1, numRegs-1))
		continue;
	      
	      if (inv_reg_alloc_order[i] < prefRegOrder)
		{
		  prefReg = i;
		  prefRegOrder = inv_reg_alloc_order[i];
		}
	    }
	}
    if (notOK && prefReg == -1)
      return -1;
    /* Get right value for the preferred register */
    numRegs = HARD_REGNO_NREGS(prefReg, PSEUDO_REGNO_MODE(currReg));
    direction = x_okay_in_direction(okay, prefReg, -1, numRegs - 1) ? -1 : 1;
    return  MIN(prefReg, prefReg + ((numRegs-1) * direction));
}
     
/* Assign registers by coloring the graph. We handle both the
   preferred class, and the alternate class, at the same time. If we
   can't place it in either, we assume we have to spill.
   We could pretty easily handle all the constraints here by simply
   excluding registers in multiple classes that are currently used.
   This would require the architecture dependent code having a routine
   to tell us "given these excluded registers, this register size, and
   this register, where can we put this?".
   
   However, i believe we can completely get rid of reload if we did this.
*/
static
void assign_regs()
{
  int i;
  int firstOKBit=-1;
  unsigned int entry;
  sbitmap coloredNodes = sbitmap_alloc(max_reg_num());  
  HARD_REG_SET *excluded = ggc_alloc_cleared(sizeof(HARD_REG_SET) * max_reg_num());
  for (i=0; i < max_reg_num();i++)
    CLEAR_HARD_REG_SET(excluded[i]);

  while (!(VARRAY_ACTIVE_SIZE(selectStack) == 0))
    {
      /* Eliminate colors of neighbors */
      unsigned int currReg = VARRAY_TOP_UINT(selectStack);
      HARD_REG_SET okColors;
      
      VARRAY_POP(selectStack);
      COPY_HARD_REG_SET(okColors, reg_class_contents[reg_preferred_class(currReg)]);
      IOR_HARD_REG_SET(okColors, reg_class_contents[reg_alternate_class(currReg)]);
      
      EXECUTE_IF_SET_IN_SBITMAP(regInfo[currReg]->adjList, 0, entry, 
      {
	unsigned int w = get_alias(entry);
	if (TEST_BIT(coloredNodes, w) || TEST_BIT(precolored, w))
	  {
	    int index = colors[w];
	    if (index >= 0)
	      {
		/* If one is a superset of the other, this should
		   already handle it, because we'll still clear the right
		   bits. */
		CLEAR_HARD_REG_BIT(okColors, index);
		AND_COMPL_HARD_REG_SET(okColors, excluded[w]);
	      }
	  }
      });
      firstOKBit = find_reg_given_constraints(okColors, currReg);
      
      if (firstOKBit == -1)
	{	  
	  /* Couldn't find a color to set -- guess we have to spill */
	  SET_BIT(spilledNodes, currReg);
	  if (debug_new_regalloc > 0)
	    fprintf(stderr, "Spilling register %d\n", currReg);
	  reg_renumber[currReg]=-1;
	}
      else
	{
	  /* We can color it. Yay! */
	  int numRegs;
	  int k;
	  
	  SET_BIT(coloredNodes, currReg);
	  colors[currReg] = firstOKBit;
	  numRegs = HARD_REGNO_NREGS(firstOKBit, PSEUDO_REGNO_MODE(currReg));
	  
	  /* Exclude the other regs this color is also using, from
	     being used by this node's neighbors. */
	  for (k = 1; k < numRegs; k++)
	    SET_HARD_REG_BIT(excluded[currReg],  colors[currReg] + k);
	  
	  if (debug_new_regalloc > 0)
	    fprintf(stderr, "Color of register %d is %d\n", currReg, colors[currReg]);  
	}
    }
  /* Now take care of coalesced variables */
  EXECUTE_IF_SET_IN_SBITMAP(coalescedNodes, 0, entry, 
  {
    unsigned int v1 = entry;
    unsigned int v2 = get_alias(v1);
    /* XXX: Why does reload bitch if we do this? */
    //df_reg_replace(dataflowAnalyser, 0, DF_REF_REG(DF_REGNO_LAST_USE(dataflowAnalyser, v1)), DF_REF_REG(DF_REGNO_LAST_USE(dataflowAnalyser, colors[v2])));
    //df_analyse(dataflowAnalyser, 0, DF_ALL);
    
    if (debug_new_regalloc > 0)
      fprintf(stderr, "Setting color of register %d to %d due to coalescing\n", v1, colors[v2]);
    colors[v1] = colors[v2];
  });
}

static 
void
rewrite_program()
{
  unsigned int i;
  void *entry;
  
  for (i=0 ; i < max_reg_num(); i++)
    reg_renumber[i] = colors[i];
  HSET_TRAVERSAL(coalescedMoves, i, entry,
  {
    /*XXX: Why does reload seem to turn this into a spill?*/
    df_insn_delete(dataflowAnalyser, BLOCK_FOR_INSN((rtx) entry), (rtx) entry);
  });
  
  build_insn_chain(get_insns());
  reload (get_insns(), 0);
}

/* Perform iterated register coalescing */  
void
perform_new_regalloc()
{
  int workDone;
  /* simplify-coalesce-freeze-spill loop */
  do {
    workDone = 0;
    if (! (sbitmap_first_set_bit(simplifyWorklist) == -1))
      {
	workDone = 1;
	if (debug_new_regalloc > 0)
	  fprintf(stderr, "simplifying\n");
	simplify();
      }
    else if (! hset_empty(worklistMoves) )
      {
	workDone = 1;
	if (debug_new_regalloc > 0)
	  fprintf(stderr, "coalescing\n");
	coalesce();
      }
    else if (! (sbitmap_first_set_bit(freezeWorklist) == -1))
      {
	workDone = 1;
	if (debug_new_regalloc > 0)
	  fprintf(stderr, "freezing\n");
	freeze();
      }
    else if (! (sbitmap_first_set_bit(spillWorklist) == -1))
      {
	workDone = 1;
	if (debug_new_regalloc > 0)
	  fprintf(stderr, "spilling\n");
	select_spill();
      }
    else
      {
	if (debug_new_regalloc > 0)
	  fprintf(stderr, "nothing\n");
      }
    
  } while(workDone);  
  /* Now assign the registers (color the graph) */
  assign_regs();
  
  /* If we spilled */
  if (! (sbitmap_first_set_bit(spilledNodes) == -1))
    {
      /* FIXME: Insert spill code insertion here */
      rewrite_program();
      ggc_collect();
      /* FIXME: Then iterate */
      // finish_new_regalloc();
      // init_new_regalloc();
    }
  else
    rewrite_program();
  
  
  
}

/* Perform register allocation */
void 
perform_new_regalloc_init()
{
  int i;
  sbitmap initial;
  sbitmap moveCandidates;
  
  basic_block currblock;
  rtx currinsn;
  
  initial = sbitmap_alloc(max_reg_num());
  sbitmap_zero(initial);
  
  /*
    Do dataflow analysis on all blocks.
  */
  df_analyse(dataflowAnalyser, 0, DF_ALL | DF_HARD_REGS);
  
  /* Place register candidates into initial, and setup interference
     graph nodes for them */
  for (i=0; i < max_reg_num(); i++)
    if ((HARD_REGISTER_NUM_P(i) && is_reg_candidate(i)) || (HARD_REGISTER_NUM_P(i) && DF_REGNO_LAST_USE(dataflowAnalyser, i) != NULL))
      {
	regInfo[i] = ig_node_new();
	regInfo[i]->degree = INT_MAX; /* If they have more than
					 INT_MAX registers, they
					 probably won't have much
					 trouble with register
					 allocation anyway */
	
	colors[i] = i;
	SET_BIT(precolored, i);
	if (debug_new_regalloc > 1)
	  fprintf(stderr, "Inserting precolored node for hard reg %d\n", i);	
      }
    else if (is_reg_candidate(i))
      {
	if (debug_new_regalloc > 1)
	  fprintf(stderr, "Inserting new candidate %d\n", i);
	SET_BIT(initial, i);
	regInfo[i] = ig_node_new();
      }
  if (debug_new_regalloc > 0) 
    {
      fprintf(stderr, "Initial set:");
      
      dump_sbitmap(stderr, initial);
      fprintf(stderr, "Precolored set:");
      
      dump_sbitmap(stderr, precolored);
    }
  

  moveCandidates = sbitmap_alloc(max_reg_num());
  sbitmap_zero(moveCandidates);
  
  sbitmap_a_or_b(moveCandidates, moveCandidates, initial);
  sbitmap_a_or_b(moveCandidates, moveCandidates, precolored);
  
  /* Process all the basic blocks */
  for (i = 0; i < n_basic_blocks; i++)
    {
      sbitmap live;
      int j;
      
      live = sbitmap_alloc(max_reg_num());
      sbitmap_zero(live);
      currblock = BASIC_BLOCK(i);
      
      /* Find all move statements in this block */
      for (currinsn = currblock->head; currinsn && currinsn !=
	     currblock->end; currinsn = next_nonnote_insn(currinsn))
	{
	  if (is_candidate_move(currinsn, moveCandidates))
	    {
	      if (debug_new_regalloc > 1)
		{
		  fprintf(stderr, "Inserting new move candidate:");
		  print_rtl(stderr, PATTERN(currinsn));		  
		  fprintf(stderr, "\n");
		}
	      
	      add_move(REGNO(SET_DEST(PATTERN(currinsn))), currinsn);
	      add_move(REGNO(SET_SRC(PATTERN(currinsn))), currinsn);
	      hset_insert(worklistMoves, currinsn);
	    }
	}
      /* Was lr_def */
      sbitmap_a_or_b(live, live, DF_BB_INFO(dataflowAnalyser, currblock)->lr_out);
      
      /* Work backwards through instructions */
      for (currinsn = prev_nonnote_insn(currblock->end); currinsn && currinsn != currblock->head; currinsn = prev_nonnote_insn(currinsn))
	{
	  struct df_link *currdef;
	  struct df_link *curruse;
	  
	  currdef = DF_INSN_DEFS(dataflowAnalyser, currinsn);
	  curruse = DF_INSN_USES(dataflowAnalyser, currinsn);
	  for (; currdef; currdef = currdef->next)
	    EXECUTE_IF_SET_IN_SBITMAP(live, 0, j,
	    {
	      if (debug_new_regalloc > 1)
		fprintf(stderr, "Inserting edge from %d to %d\n", DF_REF_REGNO(currdef->ref), j);
	      
	      add_edge(DF_REF_REGNO(currdef->ref), j);
	    });
	  for (currdef = DF_INSN_DEFS(dataflowAnalyser, currinsn); currdef; currdef = currdef->next)
	      RESET_BIT(live, DF_REF_REGNO(currdef->ref));
	  for (; curruse; curruse = curruse->next)
	    SET_BIT(live, DF_REF_REGNO(curruse->ref));
	}
    }
  /* XXX: We could add a macro that can be defined to let
     architectures add extra interference edges if necessary for very
     weird constraints. It's really no skin off our back, or theirs. */
  make_worklist(initial);
  sbitmap_free(moveCandidates);
  sbitmap_free(initial);
  
  compute_bb_for_insn (get_max_uid ()); 
}

/* Initialize and call the new register allocator */
void
init_new_regalloc()
{
  int i;
  /* Initialize our variables */
  heuristic = default_heuristic;
  
  precolored = sbitmap_alloc(max_reg_num());
  sbitmap_zero(precolored);
  
  simplifyWorklist = sbitmap_alloc(max_reg_num());
  sbitmap_zero(simplifyWorklist);
  
  freezeWorklist = sbitmap_alloc(max_reg_num());
  sbitmap_zero(freezeWorklist);
  
  spillWorklist = sbitmap_alloc(max_reg_num());
  sbitmap_zero(spillWorklist);

  spilledNodes = sbitmap_alloc(max_reg_num());
  sbitmap_zero(spilledNodes);
  
  coalescedNodes = sbitmap_alloc(max_reg_num());
  sbitmap_zero(coalescedNodes);
  
  coalescedMoves = hset_new();
  constrainedMoves = hset_new();
  frozenMoves = hset_new();
  worklistMoves = hset_new();
  activeMoves = hset_new();  
  regInfo = ggc_alloc_cleared(sizeof(ig_node) * max_reg_num());
  colors = ggc_alloc_cleared(sizeof(int) * max_reg_num());
  for (i=0; i < max_reg_num();i++)
    colors[i] = -1;
  
  VARRAY_UINT_INIT(selectStack, 1, "Interference graph stack");
  dataflowAnalyser = df_init();
  allocate_reg_info (max_regno, FALSE, TRUE);
  update_equiv_regs();
  
  perform_new_regalloc_init();
  perform_new_regalloc();
  
  finish_new_regalloc();
}

/* Clean up after the new allocator */
void
finish_new_regalloc()
{
  /* Cleanup after ourselves */
  sbitmap_free(simplifyWorklist);
  sbitmap_free(freezeWorklist);
  sbitmap_free(spillWorklist);
  sbitmap_free(spilledNodes);
  sbitmap_free(coalescedNodes);
  hset_free(coalescedMoves);
  hset_free(constrainedMoves);
  hset_free(frozenMoves);
  hset_free(worklistMoves);
  hset_free(activeMoves);
  VARRAY_FREE(selectStack);
  df_finish(dataflowAnalyser);
}

